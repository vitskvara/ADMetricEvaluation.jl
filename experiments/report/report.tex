\documentclass[a4paper]{article}

%\usepackage[round]{natbib} % for better formatting of references

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{todonotes}
\usepackage[utf8]{inputenc}
%\usepackage{booktabs} % For formal tables
\usepackage[table]{xcolor}
\usepackage{arydshln} % for dashed lines in tables
%\usepackage{subcaption}
\usepackage{pgfplotstable} % for colorized tables
\usepackage{capt-of}
\usepackage{xcolor}
\usepackage{algorithm, algpseudocode}
\usepackage{dsfont} % for the indicator function notation

\usepackage[margin=0.5in]{geometry}

\begin{document}
\title{Choosing the operating threshold in evaluation of anomaly detection methods}
\maketitle

\section{Introduction}
The problem of unsupervised anomaly detection in unknown conditions requires the determination of a suitable evaluation measure. It has been shown in previous experiments that two measures based on the ROC curve - partial AUC (AUC@$p$) and true positive rate (TPR@$p$) at a given false positive rate $p$ - are more appropriate than other (e.g. AUC, F1 score, precision). 

Now there is a question -- when not given from the outside, what is the optimal operating false positive rate $p$ at which we should measure TPR@$p$ and AUC@$p$ so that we select the best model? This depends on our definition of what is the best model. We may want to choose such a $p$ at which the tested model performance is most distinguishable. We may want it to choose a model that is the most robust with respect to differences in testing and validation/application data. 

\section{Discriminability criteria}
The experimental design is following. There are $M$ different models, each with a set of $\theta_{i} = \lbrace \theta_{ij} \rbrace_{j=1}^{I_i}$ hyperparameter settings for a total of $N=\sum_{i=1}^M I_i$ different model/hyperparameter combinations. We split a dataset into a training and testing subsets. We train the model on the training subset and evaluate the measures on the testing subset. This training and valdiation loop is done in a $k$-fold validation scheme, that is the split is done $k$ times for each basic dataset. This results in a total of $k\cdot K$ experiments. In our basic experiments, we have $M=4, k=10$. The models and hyperparameters settings are summarized in \ref{tab:hyperparams}. Theres is a total of $k \sum_{i=1}^M I_i = 10 \cdot (27 + 3 + 3 + 9) = 420$ experiments done for each dataset. 

\input{tables/hyperparams.tex}

The discriminability criteria are used to tell us how different the model performance is at different false positive rates $p$. We believe choosing such a value of $p$ at which the models are most discriminable (there are the largest differences in their performance) leads to a more robust performance measure. Alternatively, we can choose the lowest value of $p$ at which the models are already discriminable. There are different statistical tests that compare population means and variances and can be used for this.

\subsection{Welch's t-test}
Also called unequal variances t-test \cite{welch1947generalization}. It is a test to decide whether two populations have equal means. It does not need the assumption of equality of variances. Also, the population sizes can be different. However, assumption of normality is still needed. The test statistic for comparing two populations $(\mu_i, \sigma_i)$ and $(\mu_j, \sigma_j)$ of sizes $N_i,N_j$ is
\begin{equation}
	t^W_{ij} = \frac{\mu_i - \mu_j}{\sqrt{\sigma_i^2/N_i + \sigma_j^2/N_j}}.
\end{equation}
The hypothesis $H_0$ is $\mu_i = \mu_j$. Under $H_0$, $t^W_{ij}$ is from the t-distribution with $\nu$ degrees of freedom, where 
\begin{equation}
	\nu = \frac{\left( \sigma_i^2/N_i + \sigma_j^2/N_j\right)^2}{\frac{\sigma_i^4}{N^2_i(N_i-1)} + \frac{\sigma_j^4}{N^2_j(N_j-1)}}.
\end{equation}
We apply a two-tailed test on a confidence level $\alpha$, where the critical value $t^W_c = f_q^\text{T}(\nu, 1-\alpha/2)$, where $f_q^T$ is the quantile function of the Student's t-distribution. If $t^W_{ij} \notin (-t^W_c, t^W_c)$, we reject $H_0$. The p-value of Welch statistic is
\begin{equation}
	p^W = 1-\text{cdf}^\text{T}(\nu, t^W_{ij}),
\end{equation}
where $\text{cdf}^\text{T}$ is the cumulative distribution function of the Student's t-distribution. TODO: add the section for multiple comaprison as in \cite{welch1951comparison}.

\subsection{Tukey's test}
Assumes normality and homogenity of variance across groups. The test statistic is
\begin{equation}
	q^s_{ij} = \frac{|\mu_i - \mu_j|}{\sqrt{\frac{MSW}{k}}},
\end{equation}
where $MSW$ should be the mean squares within. We interpret it as the mean variance across the experiment
\begin{equation}
	MSW = \frac{1}{N}\sum_i \sigma^2_i,
\end{equation}
where $N$ is the total number of populations (comapred model/hyperparameter combinations) and $k$ is the number of samples (coming from $k$-fold crossvalidation). The test statistic is compared to the studentized range distribution with parameters $\nu = N(k-1)$ (degrees of freedom) and $N$.




\section{Results}
The tables \ref{tab:measure_comparison_full_0_by_models_tukey_mean}--\ref{tab:measure_comparison_full_0_by_models_welch_median} summarize an experiment that show us how robust a measure is with respect to the others. In other words, this is an answer to the question "How much worse in terms of measure A is model selected using measure B than that selected by using A?", where A is the column measure and B is the row measure. We select a model (one out of $\sum_{n=1}^N I_n$) that performs (on average over the $k$ folds) the best on the testing dataset using the row measure. Then we look at the value of the column measure of the same model $x_{base}$ and compare it to the best value of the column measure across all models $x_{max}$. Then we compute the relative measure loss $(x_{max}-x_{base})/x_{max}$.  The table entries are means over all datasets.

For \emph{welch_mean, welch_median} we compute all pairwise tests

\input{tables/measure_comparison_full_0_by_models_tukey_mean_alpha-5.tex}
\input{tables/measure_comparison_full_0_by_models_tukey_median_alpha-5.tex}
\input{tables/measure_comparison_full_0_by_models_tukey_q_alpha-5.tex}
\input{tables/measure_comparison_full_0_by_models_welch_mean_alpha-5.tex}
\input{tables/measure_comparison_full_0_by_models_welch_median_alpha-5.tex}

\section{What are better alternatives to AUC?}
It seems that the better alterantives for AUC are AUC@$p$ and TPR@$p$. The first one is more preferable, since it is more robust due to being an integral. Also, it brings more discriminability at higher values of $p$. This is due to the fact that at a given $p$, it is more likely that two ROC curves will have the same value of TPR as opposed to the whole integral up to $p$, where even a difference in a single (FPR,TPR) pair leads to two different values of AUC@$p$.

\bibliographystyle{acm}
\bibliography{bibliography}

\end{document}
